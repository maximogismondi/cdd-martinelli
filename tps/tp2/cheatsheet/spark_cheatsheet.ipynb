{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d37fca",
   "metadata": {},
   "source": [
    "# Spark Cheat Sheet\n",
    "\n",
    "This cheat sheet provides a quick reference to common Apache Spark commands and operations. The idea is to have a handy guide for frequently used commands and concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe11e82",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### What is Spark?\n",
    "\n",
    "Spark is an engine to process data in a distributed way. It can be used in different languages, including Scala, Python (PySpark), Java, and R.\n",
    "\n",
    "\n",
    "### Why Spark?\n",
    "\n",
    "The purpose of Spark is to process datasets that are too large to fit into the memory of a single machine. It does this by distributing the data and computations across a cluster of machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c13c01",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "from itertools import combinations\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6c4cf",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Use `reduceByKey` instead of `groupByKey`** when possible - it's more efficient\n",
    "2. **Cache RDDs** that will be reused multiple times\n",
    "3. **Use broadcast variables** for small lookup tables\n",
    "4. **Filter early** in your pipeline to reduce data size\n",
    "5. **Avoid `collect()`** on large datasets - use `take()` or aggregations instead\n",
    "6. **Use `getattr()`** for safer field access when dealing with inconsistent data\n",
    "7. **Normalize and validate** data early in the pipeline\n",
    "8. **Be careful with joins** - they can be expensive. Consider broadcast joins for small datasets\n",
    "9. **Use meaningful variable names** and add comments for complex transformations\n",
    "10. **Test on small samples** before running on full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77628c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow example\n",
    "# 1. Read data\n",
    "orders_df = spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(\"products.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2. Clean and transform\n",
    "orders_cleaned = orders_df.rdd.filter(\n",
    "    lambda r: r.product_id is not None and r.quantity is not None\n",
    ").map(\n",
    "    lambda r: (str(r.product_id), int(r.quantity))\n",
    ")\n",
    "\n",
    "# 3. Aggregate\n",
    "qty_by_product = orders_cleaned.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# 4. Get top N\n",
    "top_10 = qty_by_product.takeOrdered(10, key=lambda kv: -kv[1])\n",
    "\n",
    "# 5. Enrich with product names (join)\n",
    "products_kv = products_df.rdd.map(lambda r: (str(r.product_id), r.product_name))\n",
    "top_10_rdd = spark.sparkContext.parallelize(top_10)\n",
    "result = top_10_rdd.join(products_kv).map(\n",
    "    lambda kv: {\"name\": kv[1][1], \"quantity\": kv[1][0]}\n",
    ").collect()\n",
    "\n",
    "# 6. Display\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ba18a",
   "metadata": {},
   "source": [
    "## Complete Example: Top Products by Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results and convert to pandas for display\n",
    "results = rdd.collect()\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "if not df_results.empty:\n",
    "    df_results = df_results.sort_values(by=[\"column1\", \"column2\"]).reset_index(drop=True)\n",
    "    display(df_results)\n",
    "else:\n",
    "    print(\"No results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a955e6",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "### Convert to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average by group, then join back\n",
    "# Step 1: Calculate average by brand\n",
    "by_brand = products.map(lambda x: (x[\"brand\"], (x[\"stock\"], 1)))\n",
    "brand_totals = by_brand.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "brand_avg = brand_totals.mapValues(lambda s: s[0] / s[1] if s[1] else 0.0)\n",
    "\n",
    "# Step 2: Join products with their brand average\n",
    "products_kv = products.map(lambda x: (x[\"brand\"], x))\n",
    "joined = products_kv.join(brand_avg)  # (brand, (product, avg_stock))\n",
    "\n",
    "# Step 3: Filter based on comparison with average\n",
    "high_stock = joined.filter(lambda kv: kv[1][0][\"stock\"] >= 1.2 * kv[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c4376",
   "metadata": {},
   "source": [
    "### Multi-Step Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage after aggregation\n",
    "result = aggregated.map(lambda kv: {\n",
    "    \"key\": kv[0],\n",
    "    \"total\": kv[1][\"total\"],\n",
    "    \"active\": kv[1][\"active\"],\n",
    "    \"active_percent\": f\"{round(kv[1]['active'] / kv[1]['total'] * 100, 2)}%\" if kv[1][\"total\"] > 0 else \"0.0%\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fc214",
   "metadata": {},
   "source": [
    "### Calculate Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize boolean values\n",
    "def normalize_bool(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val).strip().lower()\n",
    "    if s in (\"true\", \"1\", \"yes\", \"y\"):\n",
    "        return True\n",
    "    if s in (\"false\", \"0\", \"no\", \"n\"):\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "# Validate against allowed set\n",
    "allowed_values = {\"REGULAR\", \"PREMIUM\", \"BUDGET\"}\n",
    "\n",
    "def normalize_from_set(val, allowed_set):\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val).strip().upper()\n",
    "    return s if s in allowed_set else None\n",
    "\n",
    "# Apply normalization\n",
    "cleaned = rdd.map(lambda r: {\n",
    "    \"segment\": normalize_from_set(r.segment, allowed_values),\n",
    "    \"is_active\": normalize_bool(r.is_active)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96ad11",
   "metadata": {},
   "source": [
    "### Normalize and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d767fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract state code from address\n",
    "STATE_REGEX = r\",\\s*([A-Z]{2})\\s+\\d{5}\"\n",
    "\n",
    "def extract_state(address):\n",
    "    if not address:\n",
    "        return None\n",
    "    match = re.search(STATE_REGEX, address)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Extract zip code\n",
    "def extract_zip(address):\n",
    "    if not address:\n",
    "        return None\n",
    "    match = re.search(r\"(\\d{5})$\", address)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Apply extraction\n",
    "states = rdd.map(lambda r: extract_state(r.address))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d546c2",
   "metadata": {},
   "source": [
    "## Common Patterns\n",
    "\n",
    "### Extract Data with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe48a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast small datasets to all workers for efficient lookups\n",
    "top_products_set = set([1, 2, 3, 4, 5])\n",
    "broadcast_products = spark.sparkContext.broadcast(top_products_set)\n",
    "\n",
    "# Use broadcast variable in transformations\n",
    "filtered = rdd.filter(lambda r: r.product_id in broadcast_products.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b85c60",
   "metadata": {},
   "source": [
    "### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache RDD in memory for reuse (important for iterative operations!)\n",
    "cached_rdd = rdd.cache()\n",
    "\n",
    "# Use cache when you'll reuse the RDD multiple times\n",
    "processed = cleaned.map(lambda x: transform(x)).cache()\n",
    "\n",
    "# Now you can use processed multiple times without recomputation\n",
    "result1 = processed.filter(lambda x: x.type == \"A\").count()\n",
    "result2 = processed.filter(lambda x: x.type == \"B\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834aefa7",
   "metadata": {},
   "source": [
    "## Optimization Techniques\n",
    "\n",
    "### Cache and Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32015db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top N elements (ascending by default)\n",
    "top_5_ascending = rdd.takeOrdered(5)\n",
    "\n",
    "# Get top N elements in descending order (use negative key)\n",
    "top_5_descending = rdd.takeOrdered(5, key=lambda x: -x)\n",
    "\n",
    "# Get top 5 by value in key-value pair\n",
    "top_5_by_value = key_value_rdd.takeOrdered(5, key=lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84f54d",
   "metadata": {},
   "source": [
    "### TakeOrdered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ade4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results to driver (be careful with large datasets!)\n",
    "results = rdd.collect()\n",
    "\n",
    "# Count number of elements\n",
    "count = rdd.count()\n",
    "\n",
    "# Take first N elements\n",
    "first_10 = rdd.take(10)\n",
    "\n",
    "# Check if RDD is empty\n",
    "is_empty = rdd.isEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7d904",
   "metadata": {},
   "source": [
    "## Actions (Trigger Computation)\n",
    "\n",
    "### Collect, Count, Take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (only matching keys)\n",
    "# rdd1: (key, value1), rdd2: (key, value2)\n",
    "joined = rdd1.join(rdd2)  # Result: (key, (value1, value2))\n",
    "\n",
    "# Left outer join (all keys from left RDD)\n",
    "left_joined = rdd1.leftOuterJoin(rdd2)  # Result: (key, (value1, Option[value2]))\n",
    "\n",
    "# Example: Join orders with customers\n",
    "orders_kv = orders.map(lambda r: (r.customer_id, r))\n",
    "customers_kv = customers.map(lambda r: (r.customer_id, r))\n",
    "joined = orders_kv.join(customers_kv)  # (customer_id, (order, customer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839687a",
   "metadata": {},
   "source": [
    "## Join Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4019a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find maximum value\n",
    "max_value = rdd.reduce(lambda a, b: a if a > b else b)\n",
    "\n",
    "# Find max by specific field\n",
    "max_discount = rdd.map(\n",
    "    lambda x: (x[\"state\"], x[\"discount\"])\n",
    ").reduce(lambda a, b: a if a[1] > b[1] else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881476b",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8086ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group values by key (returns iterator)\n",
    "grouped = key_value_rdd.groupByKey()\n",
    "\n",
    "# Convert to list for easier manipulation\n",
    "grouped_list = grouped.mapValues(list)\n",
    "\n",
    "# Example: Group products by order\n",
    "order_items = rdd.map(lambda r: (r.order_id, r.product_id)).groupByKey().mapValues(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a641a393",
   "metadata": {},
   "source": [
    "### GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum values by key\n",
    "summed = key_value_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Count occurrences by key\n",
    "counts = rdd.map(lambda x: (x.category, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Aggregate multiple metrics\n",
    "aggregated = rdd.map(\n",
    "    lambda x: (x.key, {\"count\": 1, \"sum\": x.value})\n",
    ").reduceByKey(\n",
    "    lambda a, b: {\n",
    "        \"count\": a[\"count\"] + b[\"count\"],\n",
    "        \"sum\": a[\"sum\"] + b[\"sum\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ce9b4",
   "metadata": {},
   "source": [
    "## Aggregation Operations\n",
    "\n",
    "### ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29778c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlatMap to generate multiple elements per input row\n",
    "# Useful for generating pairs from combinations\n",
    "def generate_pairs(items):\n",
    "    pairs = []\n",
    "    for prod1, prod2 in combinations(items, 2):\n",
    "        if prod1 < prod2:\n",
    "            pairs.append(((prod1, prod2), 1))\n",
    "        else:\n",
    "            pairs.append(((prod2, prod1), 1))\n",
    "    return pairs\n",
    "\n",
    "# Apply flatMap to flatten the results\n",
    "flattened = grouped_rdd.flatMap(lambda kv: generate_pairs(kv[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd40ce",
   "metadata": {},
   "source": [
    "### FlatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to transform each row into a dictionary\n",
    "mapped = rdd.map(lambda r: {\n",
    "    \"id\": r.id,\n",
    "    \"name\": r.name,\n",
    "    \"value\": float(r.value)\n",
    "})\n",
    "\n",
    "# Map to extract a single field\n",
    "values = rdd.map(lambda r: r.column_name)\n",
    "\n",
    "# Map to key-value pairs for grouping\n",
    "key_value = rdd.map(lambda r: (r.key, r.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e3519",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b96b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on a condition\n",
    "filtered = rdd.filter(lambda r: r.column_name is not None)\n",
    "\n",
    "# Filter with multiple conditions\n",
    "filtered = rdd.filter(\n",
    "    lambda r: r.status == \"REFUNDED\" and r.amount is not None\n",
    ")\n",
    "\n",
    "# Filter with getattr for safer access\n",
    "filtered = rdd.filter(\n",
    "    lambda r: getattr(r, \"field\", None) is not None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97643636",
   "metadata": {},
   "source": [
    "## Basic RDD Transformations\n",
    "\n",
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file with header and infer schema\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert DataFrame to RDD for transformations\n",
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c550d",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "### Read CSV to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313737cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57288bd0",
   "metadata": {},
   "source": [
    "## Setup SparkSession"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
