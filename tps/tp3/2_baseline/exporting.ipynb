{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626b4d48",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline\n",
    "\n",
    "Este notebook aplica todas las transformaciones de feature engineering desarrolladas durante la exploración y exporta los datasets procesados para su uso en modelos de machine learning.\n",
    "\n",
    "Postdata: Todo esto fue generado con inteligencia artificial a partir del notebook de exploración de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba8490",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07829eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import spacy\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2480d4b",
   "metadata": {},
   "source": [
    "## Cargar Modelo de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6506292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado: core_web_sm v3.8.0\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo de spacy para lematización\n",
    "model = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "print(f\"Modelo cargado: {model.meta['name']} v{model.meta['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823622e",
   "metadata": {},
   "source": [
    "## Funciones de Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70864520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    \"\"\"Limpia el texto del tweet removiendo menciones, URLs, emojis y caracteres especiales\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', '', text)                # menciones\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)     # URLs\n",
    "    text = emoji.replace_emoji(text, replace=' ')   # emojis\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)            # símbolos especiales\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # espacios extras\n",
    "    return text\n",
    "\n",
    "def lemma_filter(token):\n",
    "    \"\"\"Filtra stopwords y tokens de longitud 1\"\"\"\n",
    "    return token.lemma_ not in stop_words and len(token.lemma_) > 1\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lematiza el texto del tweet\"\"\"\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    doc = model(cleaned_text)\n",
    "    return ' '.join([token.lemma_ for token in doc if lemma_filter(token)])\n",
    "\n",
    "def add_text_features(df):\n",
    "    \"\"\"Agrega features basados en el análisis del texto\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Features básicos de texto\n",
    "    df[\"text_length\"] = df[\"text\"].str.len()\n",
    "    df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
    "    df[\"hashtag_count\"] = df[\"text\"].str.count(\"#\")\n",
    "    df[\"mention_count\"] = df[\"text\"].str.count(\"@\")\n",
    "    df[\"url_count\"] = df[\"text\"].str.count(\"http\")\n",
    "    df[\"uppercase_percentage\"] = (\n",
    "        df[\"text\"].str.findall(r\"[A-Z]\").str.len() / df[\"text_length\"]\n",
    "    )\n",
    "    df[\"punctuation_percentage\"] = (\n",
    "        df[\"text\"].str.findall(r\"[.,!?\\\"\\'()]\").str.len() / df[\"text_length\"]\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_keyword_features(df):\n",
    "    \"\"\"Limpia y procesa las keywords\"\"\"\n",
    "    df = df.copy()\n",
    "    df['keyword_clean'] = df['keyword'].str.lower().str.replace('%20', ' ')\n",
    "    return df\n",
    "\n",
    "def add_lemmatization_features(df):\n",
    "    \"\"\"Agrega el texto lematizado\"\"\"\n",
    "    df = df.copy()\n",
    "    print(\"Lematizando textos...\")\n",
    "    df['text_lemmatized'] = df['text'].apply(lemmatize_text)\n",
    "    print(\"Lematización completada!\")\n",
    "    return df\n",
    "\n",
    "def apply_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Aplica todo el pipeline de feature engineering al dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset original con columnas: id, keyword, location, text (y target si es train)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataset con todas las features agregadas\n",
    "    \"\"\"\n",
    "    print(f\"Dataset original: {df.shape}\")\n",
    "    \n",
    "    # Aplicar transformaciones\n",
    "    df = add_text_features(df)\n",
    "    print(\"✓ Features de texto agregados\")\n",
    "    \n",
    "    df = add_keyword_features(df)\n",
    "    print(\"✓ Keywords procesados\")\n",
    "    \n",
    "    df = add_lemmatization_features(df)\n",
    "    print(\"✓ Lematización completada\")\n",
    "    \n",
    "    print(f\"\\nDataset final: {df.shape}\")\n",
    "    print(f\"Nuevas columnas agregadas: {df.shape[1] - 5}\")  # 5 columnas originales (id, keyword, location, text, target)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738ee46",
   "metadata": {},
   "source": [
    "## Cargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3a2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (7613, 5)\n",
      "Test dataset: (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"../.data/raw/\"\n",
    "\n",
    "train_tweets = pd.read_csv(BASE_PATH + \"train.csv\")\n",
    "test_tweets = pd.read_csv(BASE_PATH + \"test.csv\")\n",
    "\n",
    "print(f\"Train dataset: {train_tweets.shape}\")\n",
    "print(f\"Test dataset: {test_tweets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7804c",
   "metadata": {},
   "source": [
    "## Aplicar Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3b313",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808cb4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PROCESANDO TRAIN DATASET\n",
      "==================================================\n",
      "Dataset original: (7613, 5)\n",
      "✓ Features de texto agregados\n",
      "✓ Keywords procesados\n",
      "Lematizando textos...\n",
      "Lematización completada!\n",
      "✓ Lematización completada\n",
      "\n",
      "Dataset final: (7613, 14)\n",
      "Nuevas columnas agregadas: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>uppercase_percentage</th>\n",
       "      <th>punctuation_percentage</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>957</td>\n",
       "      <td>blaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>looks like a year of writing and computers is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>blaze</td>\n",
       "      <td>look like year writing computer ahead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>5791</td>\n",
       "      <td>hail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All Hail Shadow (Hybrid Mix Feat. Mike Szuter)...</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>hail</td>\n",
       "      <td>hail shadow hybrid mix feat mike szuter youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>2588</td>\n",
       "      <td>crash</td>\n",
       "      <td>Charleston, SC</td>\n",
       "      <td>'Fatal crash reported on Johns Island' http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>crash</td>\n",
       "      <td>fatal crash report johns island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5042</th>\n",
       "      <td>7188</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>Notts</td>\n",
       "      <td>#BakeOffFriends #GBBO 'The one with the mudsli...</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>bakeofffriend gbbo mudslide guy hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666</th>\n",
       "      <td>6632</td>\n",
       "      <td>inundated</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>Already expecting to be inundated w/ articles ...</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>inundated</td>\n",
       "      <td>expect inundate article trad author pay plumme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    keyword        location  \\\n",
       "662    957      blaze             NaN   \n",
       "4073  5791       hail             NaN   \n",
       "1801  2588      crash  Charleston, SC   \n",
       "5042  7188   mudslide           Notts   \n",
       "4666  6632  inundated        Maryland   \n",
       "\n",
       "                                                   text  target  text_length  \\\n",
       "662   looks like a year of writing and computers is ...       0           75   \n",
       "4073  All Hail Shadow (Hybrid Mix Feat. Mike Szuter)...       1           89   \n",
       "1801  'Fatal crash reported on Johns Island' http://...       1           61   \n",
       "5042  #BakeOffFriends #GBBO 'The one with the mudsli...       0           74   \n",
       "4666  Already expecting to be inundated w/ articles ...       0          138   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  url_count  \\\n",
       "662           11              0              0          1   \n",
       "4073          11              2              1          1   \n",
       "1801           7              0              0          1   \n",
       "5042          13              2              0          0   \n",
       "4666          25              0              0          0   \n",
       "\n",
       "      uppercase_percentage  punctuation_percentage keyword_clean  \\\n",
       "662               0.066667                0.026667         blaze   \n",
       "4073              0.112360                0.044944          hail   \n",
       "1801              0.081967                0.049180         crash   \n",
       "5042              0.108108                0.027027      mudslide   \n",
       "4666              0.007246                0.014493     inundated   \n",
       "\n",
       "                                        text_lemmatized  \n",
       "662               look like year writing computer ahead  \n",
       "4073    hail shadow hybrid mix feat mike szuter youtube  \n",
       "1801                    fatal crash report johns island  \n",
       "5042                bakeofffriend gbbo mudslide guy hat  \n",
       "4666  expect inundate article trad author pay plumme...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PROCESANDO TRAIN DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_processed = apply_feature_engineering(train_tweets)\n",
    "train_processed.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee1d7d",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159f31c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PROCESANDO TEST DATASET\n",
      "==================================================\n",
      "Dataset original: (3263, 4)\n",
      "✓ Features de texto agregados\n",
      "✓ Keywords procesados\n",
      "Lematizando textos...\n",
      "Lematización completada!\n",
      "✓ Lematización completada\n",
      "\n",
      "Dataset final: (3263, 13)\n",
      "Nuevas columnas agregadas: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>uppercase_percentage</th>\n",
       "      <th>punctuation_percentage</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>8904</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>It's creepy seeing 676 closed. It was closed d...</td>\n",
       "      <td>121</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>creepy 676 close close snowstorm balcony overl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>2218</td>\n",
       "      <td>chemical%20emergency</td>\n",
       "      <td>? In your head ?</td>\n",
       "      <td>THE CHEMICAL BROTHERS to play The Armory in SF...</td>\n",
       "      <td>138</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.036232</td>\n",
       "      <td>chemical emergency</td>\n",
       "      <td>chemical brother play armory sf tomorrow night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>645</td>\n",
       "      <td>arsonist</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>Arson suspect linked to 30 fires caught in Nor...</td>\n",
       "      <td>86</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>arsonist</td>\n",
       "      <td>arson suspect link 30 fire catch northern cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1176</td>\n",
       "      <td>blight</td>\n",
       "      <td>Sporting capital of the World</td>\n",
       "      <td>I'm all for renewable energy but re: windfarms...</td>\n",
       "      <td>139</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.057554</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>blight</td>\n",
       "      <td>renewable energy windfarm agree abbott horribl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>7487</td>\n",
       "      <td>obliteration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Jethro_Harrup How many Hangarback Walkers doe...</td>\n",
       "      <td>117</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059829</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>obliteration</td>\n",
       "      <td>hangarback walker opponent need board infinite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               keyword                       location  \\\n",
       "2668  8904             snowstorm               Philadelphia, PA   \n",
       "682   2218  chemical%20emergency               ? In your head ?   \n",
       "199    645              arsonist                         Fresno   \n",
       "366   1176                blight  Sporting capital of the World   \n",
       "2250  7487          obliteration                            NaN   \n",
       "\n",
       "                                                   text  text_length  \\\n",
       "2668  It's creepy seeing 676 closed. It was closed d...          121   \n",
       "682   THE CHEMICAL BROTHERS to play The Armory in SF...          138   \n",
       "199   Arson suspect linked to 30 fires caught in Nor...           86   \n",
       "366   I'm all for renewable energy but re: windfarms...          139   \n",
       "2250  @Jethro_Harrup How many Hangarback Walkers doe...          117   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  url_count  \\\n",
       "2668          20              0              0          0   \n",
       "682           20              0              0          1   \n",
       "199           11              0              0          1   \n",
       "366           23              0              0          1   \n",
       "2250          17              0              1          0   \n",
       "\n",
       "      uppercase_percentage  punctuation_percentage       keyword_clean  \\\n",
       "2668              0.024793                0.024793           snowstorm   \n",
       "682               0.463768                0.036232  chemical emergency   \n",
       "199               0.058140                0.011628            arsonist   \n",
       "366               0.057554                0.014388              blight   \n",
       "2250              0.059829                0.008547        obliteration   \n",
       "\n",
       "                                        text_lemmatized  \n",
       "2668  creepy 676 close close snowstorm balcony overl...  \n",
       "682   chemical brother play armory sf tomorrow night...  \n",
       "199   arson suspect link 30 fire catch northern cali...  \n",
       "366   renewable energy windfarm agree abbott horribl...  \n",
       "2250  hangarback walker opponent need board infinite...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PROCESANDO TEST DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_processed = apply_feature_engineering(test_tweets)\n",
    "test_processed.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb8b24",
   "metadata": {},
   "source": [
    "## Exportar Datasets Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d6be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando datasets procesados...\n",
      "✓ Train guardado en: ..\\.data\\processed\\train.pkl\n",
      "✓ Test guardado en: ..\\.data\\processed\\test.pkl\n",
      "==================================================\n",
      "EXPORTACIÓN COMPLETADA\n",
      "==================================================\n",
      "\n",
      "Archivos generados:\n",
      "  - ..\\.data\\processed\\train.pkl (1.87 MB)\n",
      "  - ..\\.data\\processed\\test.pkl (0.79 MB)\n"
     ]
    }
   ],
   "source": [
    "# Crear directorio de salida si no existe\n",
    "OUTPUT_PATH = Path(\"../.data/processed/\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Exportar a pickle\n",
    "train_output = OUTPUT_PATH / \"train.pkl\"\n",
    "test_output = OUTPUT_PATH / \"test.pkl\"\n",
    "\n",
    "print(\"\\nGuardando datasets procesados...\")\n",
    "train_processed.to_pickle(train_output)\n",
    "print(f\"✓ Train guardado en: {train_output}\")\n",
    "\n",
    "test_processed.to_pickle(test_output)\n",
    "print(f\"✓ Test guardado en: {test_output}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"EXPORTACIÓN COMPLETADA\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(f\"  - {train_output} ({train_output.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"  - {test_output} ({test_output.stat().st_size / 1024 / 1024:.2f} MB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
