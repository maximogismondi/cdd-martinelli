{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd729f19",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, make_scorer, roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from emoji import demojize\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.cli import download\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de51950",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_NO_DISASTER = '#3498db'\n",
    "COLOR_DISASTER = '#e74c3c'\n",
    "COLOR_GENERAL = '#95a5a6'\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf10ff3",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"../.data/raw\")\n",
    "df = pd.read_csv(data_path / \"train.csv\")\n",
    "test_df = pd.read_csv(data_path / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = df['target'].mean()\n",
    "print(f'Shape del dataset: {df.shape}')\n",
    "print(f'Porcentaje de desastres en el target: {target_mean*100:.2f}%')\n",
    "df.sample(5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e0a64",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Texto limpio y lematizado\n",
    "    df['clean_text'] = df['text'].str.lower().apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "    \n",
    "    # Features estadísticas\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    df['char_count'] = df['text'].str.len()\n",
    "    df['stopwords_count'] = df['text'].str.lower().str.split().apply(lambda x: sum(w in STOP_WORDS for w in x))\n",
    "    df['hashtag_count'] = df['text'].str.count(r'#\\w+')\n",
    "    df['mention_count'] = df['text'].str.count(r'@\\w+')\n",
    "    df['url_count'] = df['text'].str.count(r'http\\S+|www\\S+')\n",
    "    \n",
    "    word_lengths = df['text'].str.split().apply(lambda x: np.mean([len(w) for w in x]) if len(x) > 0 else 0)\n",
    "    df['avg_word_length'] = word_lengths\n",
    "    \n",
    "    char_lengths = df['text'].str.len()\n",
    "    uppercase_counts = df['text'].str.count(r'[A-Z]')\n",
    "    df['uppercase_ratio'] = np.where(char_lengths > 0, uppercase_counts / char_lengths, 0)\n",
    "    \n",
    "    df['punct_count'] = df['text'].str.count(r'[^\\w\\s]')\n",
    "    df['number_count'] = df['text'].str.count(r'\\d+')\n",
    "    df['emoji_count'] = df['text'].apply(lambda x: len([c for c in x if c in demojize(x)]))\n",
    "    \n",
    "    # Ubicación\n",
    "    df['has_location'] = df['location'].notna().astype(int)\n",
    "    \n",
    "    # Keywords lematizadas\n",
    "    df['keyword_lemma'] = df['keyword'].fillna('none').apply(lambda x: ' '.join([token.lemma_ for token in nlp(str(x))]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Extrayendo features de train...\")\n",
    "df = extract_features(df)\n",
    "print(\"Extrayendo features de test...\")\n",
    "test_df = extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21392e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de texto: Word TF-IDF + Char TF-IDF + SVD\n",
    "word_tfidf = TfidfVectorizer(\n",
    "    max_features=3000, \n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=2000,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('word', word_tfidf),\n",
    "        ('char', char_tfidf),\n",
    "    ])),\n",
    "    ('svd', TruncatedSVD(n_components=200, random_state=SEED))\n",
    "])\n",
    "\n",
    "print(\"Generando features densas con SVD...\")\n",
    "X_text_dense = text_pipeline.fit_transform(df['clean_text'])\n",
    "X_text_dense_test = text_pipeline.transform(test_df['clean_text'])\n",
    "\n",
    "print(f'Shape de texto denso (SVD): {X_text_dense.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059de24",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features numéricas\n",
    "numeric_cols = ['word_count', 'char_count', 'stopwords_count', 'hashtag_count', 'mention_count', \n",
    "                'url_count', 'avg_word_length', 'uppercase_ratio', 'punct_count', 'number_count', \n",
    "                'emoji_count', 'has_location']\n",
    "\n",
    "X_numeric = df[numeric_cols].values\n",
    "X_numeric_test = test_df[numeric_cols].values\n",
    "\n",
    "# Escalar features numéricas\n",
    "scaler = StandardScaler()\n",
    "X_numeric = scaler.fit_transform(X_numeric)\n",
    "X_numeric_test = scaler.transform(X_numeric_test)\n",
    "\n",
    "# Mean Encoding para keywords\n",
    "keyword_means = df.groupby('keyword_lemma')['target'].mean()\n",
    "global_mean = df['target'].mean()\n",
    "\n",
    "train_keyword_encoded = df['keyword_lemma'].map(keyword_means).fillna(global_mean).values.reshape(-1, 1)\n",
    "test_keyword_encoded = test_df['keyword_lemma'].map(keyword_means).fillna(global_mean).values.reshape(-1, 1)\n",
    "\n",
    "# Combinar features (Sparse + Dense)\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_text_dense, X_numeric, train_keyword_encoded])\n",
    "X_submission = hstack([X_text_dense_test, X_numeric_test, test_keyword_encoded])\n",
    "y = df['target'].values\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "print(f'Shape de X_train: {X_train.shape}')\n",
    "\n",
    "# Modelo con búsqueda de hiperparámetros\n",
    "param_distributions = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    lr, \n",
    "    param_distributions, \n",
    "    n_iter=10,\n",
    "    cv=skf, \n",
    "    scoring=make_scorer(f1_score), \n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    random_state=SEED\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Mejores parámetros: {random_search.best_params_}')\n",
    "print(f'Mejor F1-Score (CV): {random_search.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8741ba70",
   "metadata": {},
   "source": [
    "## Evaluación y Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# 1. Curva ROC\n",
    "y_probs = best_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Mejor threshold\n",
    "J = tpr - fpr\n",
    "ix = np.argmax(J)\n",
    "best_thresh = thresholds[ix]\n",
    "print(f'Best Threshold: {best_thresh:.4f}')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=COLOR_DISASTER, lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color=COLOR_GENERAL, lw=2, linestyle='--')\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label=f'Best Threshold = {best_thresh:.2f}')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Matriz de Confusión\n",
    "y_pred_opt = (y_probs > best_thresh).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred_opt)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(f'Confusion Matrix (Threshold = {best_thresh:.2f})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# 3. Feature Importance (Top 20 coeficientes)\n",
    "# Recuperar nombres de features\n",
    "text_features = text_pipeline.named_steps['union'].get_feature_names_out()\n",
    "svd_cols = [f'svd_{i}' for i in range(X_text_dense.shape[1])]\n",
    "all_features = text_features + numeric_cols + ['keyword_encoded']\n",
    "\n",
    "coefs = best_model.coef_[0]\n",
    "indices = np.argsort(np.abs(coefs))[::-1][:20]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=coefs[indices], y=np.array(all_features)[indices], palette=\"viridis\")\n",
    "plt.title(\"Top 20 Feature Importance (Logistic Regression Coefficients)\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca352625",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd380fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar submission\n",
    "y_probs_sub = best_model.predict_proba(X_submission)[:, 1]\n",
    "y_pred_sub = (y_probs_sub > best_thresh).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'target': y_pred_sub})\n",
    "\n",
    "submission_path = pathlib.Path(\"../.data/submission\")\n",
    "submission_path.mkdir(parents=True, exist_ok=True)\n",
    "submission.to_csv(submission_path / \"linear_regression_final_submission.csv\", index=False)\n",
    "\n",
    "print(f\"Submission guardada en {submission_path / 'linear_regression_final_submission.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
