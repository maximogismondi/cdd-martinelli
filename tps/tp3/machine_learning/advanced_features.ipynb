{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b9966a",
   "metadata": {},
   "source": [
    "# Feature Engineering Avanzado\n",
    "\n",
    "Este notebook expande las features existentes con:\n",
    "1. **Análisis de sentimiento** usando TextBlob\n",
    "2. **Geocodificación** de locations a coordenadas (cuando sea posible)\n",
    "3. **Features lingüísticas adicionales** (emojis, capitalización, etc)\n",
    "4. **Features de intensidad** (palabras de urgencia y desastres)\n",
    "\n",
    "Estos features serán utilizados en los modelos avanzados (model1 y model2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a7dc6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c8f00c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Sentiment analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Progress bar\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Constantes\n",
    "COLOR_NO_DISASTER = '#3498db'\n",
    "COLOR_DISASTER = '#e74c3c'\n",
    "COLOR_GENERAL = '#95a5a6'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c4969",
   "metadata": {},
   "source": [
    "## Cargar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc911a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../.data/processed/\")\n",
    "RAW_PATH = Path(\"../.data/raw/\")\n",
    "\n",
    "# Cargar datos procesados (ya tienen features básicas)\n",
    "train_df = pd.read_pickle(DATA_PATH / \"train.pkl\")\n",
    "test_df = pd.read_pickle(DATA_PATH / \"test.pkl\")\n",
    "\n",
    "# Cargar raw para location\n",
    "train_raw = pd.read_csv(RAW_PATH / \"train.csv\")\n",
    "test_raw = pd.read_csv(RAW_PATH / \"test.csv\")\n",
    "\n",
    "# Agregar location a los DataFrames procesados\n",
    "train_df['location'] = train_raw['location']\n",
    "test_df['location'] = test_raw['location']\n",
    "\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "print(f\"\\nColumnas actuales: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a0b8a",
   "metadata": {},
   "source": [
    "## 1. Análisis de Sentimiento con TextBlob\n",
    "\n",
    "TextBlob proporciona:\n",
    "- **Polarity**: -1 (negativo) a +1 (positivo)\n",
    "- **Subjectivity**: 0 (objetivo) a 1 (subjetivo)\n",
    "\n",
    "Los tweets sobre desastres probablemente tengan polaridad negativa y mayor subjetividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Calcula polarity y subjectivity usando TextBlob\n",
    "    \"\"\"\n",
    "    try:\n",
    "        blob = TextBlob(str(text))\n",
    "        return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "    except:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "# Calcular sentimiento\n",
    "print(\"Calculando sentimiento para train...\")\n",
    "train_sentiment = train_df['text'].progress_apply(get_sentiment)\n",
    "train_df['sentiment_polarity'] = [s[0] for s in train_sentiment]\n",
    "train_df['sentiment_subjectivity'] = [s[1] for s in train_sentiment]\n",
    "\n",
    "print(\"Calculando sentimiento para test...\")\n",
    "test_sentiment = test_df['text'].progress_apply(get_sentiment)\n",
    "test_df['sentiment_polarity'] = [s[0] for s in test_sentiment]\n",
    "test_df['sentiment_subjectivity'] = [s[1] for s in test_sentiment]\n",
    "\n",
    "print(\"\\n✅ Sentimiento agregado\")\n",
    "print(f\"Train sentiment stats:\\n{train_df[['sentiment_polarity', 'sentiment_subjectivity']].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab128f6d",
   "metadata": {},
   "source": [
    "## 2. Features Lingüísticas Avanzadas\n",
    "\n",
    "Agregar features adicionales que pueden ser útiles para clasificación:\n",
    "- Número de emojis\n",
    "- Presencia de palabras en mayúsculas (gritos)\n",
    "- Ratio de palabras únicas (lexical diversity)\n",
    "- Presencia de números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_emojis(text):\n",
    "    \"\"\"Cuenta emojis en el texto usando rangos Unicode\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return len(emoji_pattern.findall(str(text)))\n",
    "\n",
    "def count_uppercase_words(text):\n",
    "    \"\"\"Cuenta palabras completamente en mayúsculas (excluye palabras de 1 letra)\"\"\"\n",
    "    words = str(text).split()\n",
    "    return sum(1 for word in words if word.isupper() and len(word) > 1)\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    \"\"\"Ratio de palabras únicas vs total de palabras\"\"\"\n",
    "    words = str(text).lower().split()\n",
    "    if len(words) == 0:\n",
    "        return 0.0\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "def count_numbers(text):\n",
    "    \"\"\"Cuenta secuencias de números en el texto\"\"\"\n",
    "    return len(re.findall(r'\\d+', str(text)))\n",
    "\n",
    "# Aplicar features\n",
    "print(\"Calculando features lingüísticas...\")\n",
    "\n",
    "for df, name in [(train_df, 'train'), (test_df, 'test')]:\n",
    "    print(f\"\\nProcesando {name}...\")\n",
    "    df['emoji_count'] = df['text'].progress_apply(count_emojis)\n",
    "    df['uppercase_word_count'] = df['text'].progress_apply(count_uppercase_words)\n",
    "    df['lexical_diversity'] = df['text'].progress_apply(lexical_diversity)\n",
    "    df['number_count'] = df['text'].progress_apply(count_numbers)\n",
    "\n",
    "print(\"\\n✅ Features lingüísticas agregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a091752",
   "metadata": {},
   "source": [
    "## 3. Geocodificación de Locations\n",
    "\n",
    "Intentar convertir locations en coordenadas. Como esto puede ser costoso y muchas locations son inválidas,\n",
    "haremos un enfoque simplificado:\n",
    "- Identificar countries/ciudades principales\n",
    "- Asignar coordenadas aproximadas a ubicaciones conocidas\n",
    "- Usar NaN para ubicaciones desconocidas\n",
    "\n",
    "Esto puede ayudar al modelo a encontrar patrones geográficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de ubicaciones conocidas -> coordenadas aproximadas (lat, lon)\n",
    "LOCATION_COORDS = {\n",
    "    # USA\n",
    "    'usa': (37.0902, -95.7129),\n",
    "    'united states': (37.0902, -95.7129),\n",
    "    'us': (37.0902, -95.7129),\n",
    "    'new york': (40.7128, -74.0060),\n",
    "    'nyc': (40.7128, -74.0060),\n",
    "    'los angeles': (34.0522, -118.2437),\n",
    "    'california': (36.7783, -119.4179),\n",
    "    'chicago': (41.8781, -87.6298),\n",
    "    'miami': (25.7617, -80.1918),\n",
    "    'washington': (38.9072, -77.0369),\n",
    "    'boston': (42.3601, -71.0589),\n",
    "    'san francisco': (37.7749, -122.4194),\n",
    "    'seattle': (47.6062, -122.3321),\n",
    "    \n",
    "    # UK\n",
    "    'uk': (55.3781, -3.4360),\n",
    "    'united kingdom': (55.3781, -3.4360),\n",
    "    'london': (51.5074, -0.1278),\n",
    "    'england': (52.3555, -1.1743),\n",
    "    \n",
    "    # Canada\n",
    "    'canada': (56.1304, -106.3468),\n",
    "    'toronto': (43.6532, -79.3832),\n",
    "    \n",
    "    # Australia\n",
    "    'australia': (-25.2744, 133.7751),\n",
    "    'sydney': (-33.8688, 151.2093),\n",
    "    \n",
    "    # India\n",
    "    'india': (20.5937, 78.9629),\n",
    "    'mumbai': (19.0760, 72.8777),\n",
    "    \n",
    "    # Nigeria\n",
    "    'nigeria': (9.0820, 8.6753),\n",
    "    'lagos': (6.5244, 3.3792),\n",
    "    \n",
    "    # Kenya\n",
    "    'kenya': (-0.0236, 37.9062),\n",
    "    'nairobi': (-1.2864, 36.8172),\n",
    "}\n",
    "\n",
    "def geocode_location(location):\n",
    "    \"\"\"\n",
    "    Convierte location string a coordenadas (lat, lon)\n",
    "    Retorna (None, None) si no se puede geocodificar\n",
    "    \"\"\"\n",
    "    if pd.isna(location):\n",
    "        return None, None\n",
    "    \n",
    "    location_lower = str(location).lower().strip()\n",
    "    \n",
    "    # Buscar coincidencia exacta\n",
    "    if location_lower in LOCATION_COORDS:\n",
    "        return LOCATION_COORDS[location_lower]\n",
    "    \n",
    "    # Buscar coincidencia parcial\n",
    "    for key, coords in LOCATION_COORDS.items():\n",
    "        if key in location_lower:\n",
    "            return coords\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Aplicar geocodificación\n",
    "print(\"Geocodificando locations...\")\n",
    "\n",
    "for df, name in [(train_df, 'train'), (test_df, 'test')]:\n",
    "    coords = df['location'].progress_apply(geocode_location)\n",
    "    df['location_lat'] = [c[0] for c in coords]\n",
    "    df['location_lon'] = [c[1] for c in coords]\n",
    "    \n",
    "    # Feature binaria: tiene location válida\n",
    "    df['has_valid_location'] = (~df['location_lat'].isna()).astype(int)\n",
    "    \n",
    "    geocoded_pct = df['has_valid_location'].mean() * 100\n",
    "    print(f\"{name}: {geocoded_pct:.1f}% locations geocodificadas\")\n",
    "\n",
    "print(\"\\n✅ Geocodificación completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359b00e",
   "metadata": {},
   "source": [
    "## 4. Features de Intensidad de Emergencia\n",
    "\n",
    "Palabras que sugieren urgencia o severidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras de urgencia/emergencia\n",
    "URGENCY_WORDS = ['urgent', 'emergency', 'help', 'sos', 'alert', 'warning', 'danger', \n",
    "                 'critical', 'severe', 'breaking', 'now', 'immediately', 'asap']\n",
    "\n",
    "DISASTER_INTENSITY_WORDS = ['devastating', 'catastrophic', 'massive', 'huge', 'major',\n",
    "                            'deadly', 'fatal', 'tragedy', 'victims', 'casualties',\n",
    "                            'destroyed', 'collapsed', 'killed', 'injured']\n",
    "\n",
    "def count_word_list(text, word_list):\n",
    "    \"\"\"Cuenta cuántas palabras de la lista aparecen en el texto\"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    return sum(1 for word in word_list if word in text_lower)\n",
    "\n",
    "# Aplicar\n",
    "for df in [train_df, test_df]:\n",
    "    df['urgency_word_count'] = df['text'].apply(lambda x: count_word_list(x, URGENCY_WORDS))\n",
    "    df['intensity_word_count'] = df['text'].apply(lambda x: count_word_list(x, DISASTER_INTENSITY_WORDS))\n",
    "\n",
    "print(\"✅ Features de intensidad agregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24002689",
   "metadata": {},
   "source": [
    "## 5. Resumen de Nuevas Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245963ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar nuevas features\n",
    "new_features = [\n",
    "    'sentiment_polarity',\n",
    "    'sentiment_subjectivity',\n",
    "    'emoji_count',\n",
    "    'uppercase_word_count',\n",
    "    'lexical_diversity',\n",
    "    'number_count',\n",
    "    'location_lat',\n",
    "    'location_lon',\n",
    "    'has_valid_location',\n",
    "    'urgency_word_count',\n",
    "    'intensity_word_count'\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NUEVAS FEATURES AGREGADAS\".center(60))\n",
    "print(\"=\" * 60)\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nTotal nuevas features: {len(new_features)}\")\n",
    "print(f\"Features anteriores: {train_df.shape[1] - len(new_features)}\")\n",
    "print(f\"Features totales ahora: {train_df.shape[1]}\")\n",
    "\n",
    "print(\"\\nEstadísticas de nuevas features (train):\")\n",
    "print(train_df[new_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60860230",
   "metadata": {},
   "source": [
    "## 6. Exportar Datos Enriquecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en nueva ubicación\n",
    "OUTPUT_PATH = Path(\"../.data/processed/\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_output = OUTPUT_PATH / \"train_advanced.pkl\"\n",
    "test_output = OUTPUT_PATH / \"test_advanced.pkl\"\n",
    "\n",
    "train_df.to_pickle(train_output)\n",
    "test_df.to_pickle(test_output)\n",
    "\n",
    "print(\"✅ Datos guardados:\")\n",
    "print(f\"  - {train_output.absolute()}\")\n",
    "print(f\"  - {test_output.absolute()}\")\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Test: {test_df.shape}\")\n",
    "\n",
    "# Verificar que se guardaron correctamente\n",
    "import os\n",
    "if os.path.exists(train_output):\n",
    "    size_mb = os.path.getsize(train_output) / (1024 * 1024)\n",
    "    print(f\"\\n✓ train_advanced.pkl: {size_mb:.2f} MB\")\n",
    "if os.path.exists(test_output):\n",
    "    size_mb = os.path.getsize(test_output) / (1024 * 1024)\n",
    "    print(f\"✓ test_advanced.pkl: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ced36",
   "metadata": {},
   "source": [
    "## Análisis Exploratorio de Nuevas Features\n",
    "\n",
    "Veamos cómo se relacionan con el target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Comparar sentimiento por target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Polarity\n",
    "axes[0, 0].hist([train_df[train_df['target']==0]['sentiment_polarity'],\n",
    "                 train_df[train_df['target']==1]['sentiment_polarity']],\n",
    "                label=['No Disaster', 'Disaster'], bins=30, alpha=0.7,\n",
    "                color=[COLOR_NO_DISASTER, COLOR_DISASTER])\n",
    "axes[0, 0].set_xlabel('Sentiment Polarity')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribución de Polarity por Target', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Subjectivity\n",
    "axes[0, 1].hist([train_df[train_df['target']==0]['sentiment_subjectivity'],\n",
    "                 train_df[train_df['target']==1]['sentiment_subjectivity']],\n",
    "                label=['No Disaster', 'Disaster'], bins=30, alpha=0.7,\n",
    "                color=[COLOR_NO_DISASTER, COLOR_DISASTER])\n",
    "axes[0, 1].set_xlabel('Sentiment Subjectivity')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribución de Subjectivity por Target', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Urgency words\n",
    "urgency_comparison = train_df.groupby('target')['urgency_word_count'].mean()\n",
    "axes[1, 0].bar([0, 1], urgency_comparison.values, \n",
    "               color=[COLOR_NO_DISASTER, COLOR_DISASTER], alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Target')\n",
    "axes[1, 0].set_ylabel('Promedio Palabras Urgencia')\n",
    "axes[1, 0].set_title('Promedio de Palabras de Urgencia por Target', fontweight='bold')\n",
    "axes[1, 0].set_xticks([0, 1])\n",
    "axes[1, 0].set_xticklabels(['No Disaster', 'Disaster'])\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Intensity words\n",
    "intensity_comparison = train_df.groupby('target')['intensity_word_count'].mean()\n",
    "axes[1, 1].bar([0, 1], intensity_comparison.values,\n",
    "               color=[COLOR_NO_DISASTER, COLOR_DISASTER], alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Target')\n",
    "axes[1, 1].set_ylabel('Promedio Palabras Intensidad')\n",
    "axes[1, 1].set_title('Promedio de Palabras de Intensidad por Target', fontweight='bold')\n",
    "axes[1, 1].set_xticks([0, 1])\n",
    "axes[1, 1].set_xticklabels(['No Disaster', 'Disaster'])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats por target\n",
    "print(\"\\nEstadísticas por Target:\")\n",
    "print(\"\\nNo Disaster (0):\")\n",
    "print(train_df[train_df['target']==0][new_features].describe().loc[['mean', 'std']])\n",
    "print(\"\\nDisaster (1):\")\n",
    "print(train_df[train_df['target']==1][new_features].describe().loc[['mean', 'std']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
