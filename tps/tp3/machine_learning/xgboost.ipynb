{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef210400",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) es un modelo de aprendizaje supervisado basado en árboles de decisión que utiliza el algoritmo de boosting para mejorar la precisión de las predicciones. A grandes rasgos, está conformado por una serie de árboles de decisión que se construyen secuencialmente, donde cada árbol intenta corregir los errores del árbol anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1952b7",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Vamos a usar las siguientes librerías para la implementación del modelo XGBoost con nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a53177",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost spacy emoji scikit-learn pandas numpy\n",
    "\n",
    "# Standar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "# ML    \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from emoji import demojize\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.cli import download\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3fe3e7",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f83f7131",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_NO_DISASTER = '#3498db'\n",
    "COLOR_DISASTER = '#e74c3c'\n",
    "COLOR_GENERAL = '#95a5a6'\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fc1a3",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "Vamos a importar los datos crudos y preprocesarlos para poder entrenar el modelo XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70e374ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"../.data/raw\")\n",
    "\n",
    "# Datos\n",
    "df = pd.read_csv(data_path / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf7de435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape del dataset: (7613, 5)\n",
      "Porcentaje de desastres en el target: 42.97%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mean = df['target'].mean()\n",
    "\n",
    "print(f'Shape del dataset: {df.shape}')\n",
    "print(f'Porcentaje de desastres en el target: {target_mean*100:.2f}%')\n",
    "\n",
    "columns = ['id', 'keyword', 'location', 'text', 'target']\n",
    "df = df[columns]\n",
    "df.sample(5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ba505",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Vamos a agregar las siguientes features al dataset:\n",
    "\n",
    "### Features estadísticas del texto\n",
    "\n",
    "- Cantidad de palabras\n",
    "- Cantidad de caracteres\n",
    "- Cantidad de stopwords\n",
    "- Cantidad de hashtags\n",
    "- Cantidad de menciones\n",
    "- Cantidad de URLs\n",
    "- Promedio de longitud de las palabras\n",
    "- Promedio de palabras en mayúsculas\n",
    "- Cantidad de signos de puntuación\n",
    "- Cantidad de números\n",
    "- Cantidad de emojis\n",
    "\n",
    "### Features de sentimiento\n",
    "\n",
    "- Sentimiento positivo\n",
    "- Sentimiento negativo\n",
    "- Sentimiento neutral\n",
    "\n",
    "### Features de TF-IDF\n",
    "\n",
    "Vamos a usar TF-IDF para convertir el texto en una representación numérica que pueda ser utilizada por el modelo XGBoost.\n",
    "\n",
    "### Encoding para las keywords\n",
    "\n",
    "Al ser una feature categórica, primero vamos a lematizar las keywords y luego aplicarle:\n",
    "\n",
    "- One-Hot Encoding\n",
    "- Mean Encoding\n",
    "\n",
    "### Ubicación geográfica\n",
    "\n",
    "Como ya vimos que la feature de ubicación tiene muchos valores falsos, vamos a usar un encoder geográfico que deteermine si la ubicación es válida o no:\n",
    "\n",
    "- GPE de spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba3e9f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>number_count</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>has_location</th>\n",
       "      <th>valid_location</th>\n",
       "      <th>keyword_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "      <td>so you have a new weapon that can cause un - i...</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>destruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "      <td>the f$&amp;amp;@ing thing I do for # gishwhe just ...</td>\n",
       "      <td>21</td>\n",
       "      <td>119</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>0.100840</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "      <td>dt @georgegalloway : rt @galloway4mayor : ûït...</td>\n",
       "      <td>15</td>\n",
       "      <td>125</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aftershock back to school kick off be great . ...</td>\n",
       "      <td>21</td>\n",
       "      <td>114</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.476190</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>aftershock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "      <td>in response to trauma child of addict develop ...</td>\n",
       "      <td>17</td>\n",
       "      <td>105</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.235294</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>trauma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2644  So you have a new weapon that can cause un-ima...       1   \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0   \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1   \n",
       "132   Aftershock back to school kick off was great. ...       0   \n",
       "6845  in response to trauma Children of Addicts deve...       0   \n",
       "\n",
       "                                             clean_text  word_count  \\\n",
       "2644  so you have a new weapon that can cause un - i...          11   \n",
       "2227  the f$&amp;@ing thing I do for # gishwhe just ...          21   \n",
       "5448  dt @georgegalloway : rt @galloway4mayor : ûït...          15   \n",
       "132   aftershock back to school kick off be great . ...          21   \n",
       "6845  in response to trauma child of addict develop ...          17   \n",
       "\n",
       "      char_count  stopwords_count  hashtag_count  mention_count  url_count  \\\n",
       "2644          66                6              0              0          0   \n",
       "2227         119                9              1              2          0   \n",
       "5448         125                3              0              2          1   \n",
       "132          114               11              0              0          0   \n",
       "6845         105                6              0              0          0   \n",
       "\n",
       "      avg_word_length  uppercase_ratio  punct_count  number_count  \\\n",
       "2644         5.090909         0.015152            2             0   \n",
       "2227         4.714286         0.100840           10             0   \n",
       "5448         7.400000         0.120000           13             3   \n",
       "132          4.476190         0.026316            3             0   \n",
       "6845         5.235294         0.019048            3             1   \n",
       "\n",
       "      emoji_count  has_location  valid_location keyword_lemma  \n",
       "2644           66             0               0   destruction  \n",
       "2227          119             0               0        deluge  \n",
       "5448          125             1               1        police  \n",
       "132           114             0               0    aftershock  \n",
       "6845          105             1               1        trauma  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Texto limpio y lematizado (SIN eliminar stopwords - son importantes para contexto)\n",
    "    df['clean_text'] = df['text'].str.lower().apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "    \n",
    "    # Features estadísticas\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    df['char_count'] = df['text'].str.len()\n",
    "    df['stopwords_count'] = df['text'].str.lower().str.split().apply(lambda x: sum(w in STOP_WORDS for w in x))\n",
    "    df['hashtag_count'] = df['text'].str.count(r'#\\w+')\n",
    "    df['mention_count'] = df['text'].str.count(r'@\\w+')\n",
    "    df['url_count'] = df['text'].str.count(r'http\\S+|www\\S+')\n",
    "    \n",
    "    word_lengths = df['text'].str.split().apply(lambda x: np.mean([len(w) for w in x]) if len(x) > 0 else 0)\n",
    "    df['avg_word_length'] = word_lengths\n",
    "    \n",
    "    char_lengths = df['text'].str.len()\n",
    "    uppercase_counts = df['text'].str.count(r'[A-Z]')\n",
    "    df['uppercase_ratio'] = np.where(char_lengths > 0, uppercase_counts / char_lengths, 0)\n",
    "    \n",
    "    df['punct_count'] = df['text'].str.count(r'[^\\w\\s]')\n",
    "    df['number_count'] = df['text'].str.count(r'\\d+')\n",
    "    df['emoji_count'] = df['text'].apply(lambda x: len([c for c in x if c in demojize(x)]))\n",
    "    \n",
    "    # Ubicación\n",
    "    df['has_location'] = df['location'].notna().astype(int)\n",
    "    df['valid_location'] = np.where(\n",
    "        df['location'].notna(),\n",
    "        df['location'].apply(lambda x: any(ent.label_ == 'GPE' for ent in nlp(str(x)).ents)),\n",
    "        False\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Keywords lematizadas\n",
    "    df['keyword_lemma'] = df['keyword'].fillna('none').apply(lambda x: ' '.join([token.lemma_ for token in nlp(str(x))]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = extract_features(df)\n",
    "df.sample(5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a48a2707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando features densas con SVD...\n",
      "Shape de texto denso (SVD): (7613, 400)\n",
      "Shape de features numéricas: (7613, 13)\n",
      "Shape de texto denso (SVD): (7613, 400)\n",
      "Shape de features numéricas: (7613, 13)\n"
     ]
    }
   ],
   "source": [
    "# Pipeline de texto: Word TF-IDF + Char TF-IDF + SVD para densificar\n",
    "word_tfidf = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2  # Ignora palabras en menos de 2 tweets\n",
    ")\n",
    "\n",
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),  # Secuencias de 3-5 caracteres\n",
    "    max_features=3000,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('word', word_tfidf),\n",
    "        ('char', char_tfidf),\n",
    "    ])),\n",
    "    ('svd', TruncatedSVD(n_components=400, random_state=SEED))\n",
    "])\n",
    "\n",
    "print(\"Generando features densas con SVD...\")\n",
    "X_text_dense = text_pipeline.fit_transform(df['clean_text'])\n",
    "\n",
    "print(f'Shape de texto denso (SVD): {X_text_dense.shape}')\n",
    "\n",
    "# Features numéricas\n",
    "feature_cols = ['word_count', 'char_count', 'stopwords_count', 'hashtag_count', 'mention_count', \n",
    "                'url_count', 'avg_word_length', 'uppercase_ratio', 'punct_count', 'number_count', \n",
    "                'emoji_count', 'has_location', 'valid_location']\n",
    "\n",
    "print(f'Shape de features numéricas: {df[feature_cols].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d4edec",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ae50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de X_train: (6090, 414)\n",
      "Shape de X_test: (1523, 414)\n",
      "Tipo de matriz: <class 'numpy.ndarray'> (densa)\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    }
   ],
   "source": [
    "# Split ANTES para evitar data leakage\n",
    "X_numeric = df[['word_count', 'char_count', 'stopwords_count', 'hashtag_count', 'mention_count', \n",
    "                'url_count', 'avg_word_length', 'uppercase_ratio', 'punct_count', 'number_count', \n",
    "                'emoji_count', 'has_location', 'valid_location']].values\n",
    "\n",
    "y = df['target'].values\n",
    "\n",
    "# Split train/test\n",
    "X_train_idx, X_test_idx = train_test_split(np.arange(len(df)), test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "# Mean Encoding SIN LEAKAGE (solo Mean Encoding, sin One-Hot)\n",
    "train_data = df.iloc[X_train_idx]\n",
    "test_data = df.iloc[X_test_idx]\n",
    "\n",
    "keyword_means = train_data.groupby('keyword_lemma')['target'].mean()\n",
    "global_mean = train_data['target'].mean()\n",
    "\n",
    "train_keyword_encoded = train_data['keyword_lemma'].map(keyword_means).fillna(global_mean).values.reshape(-1, 1)\n",
    "test_keyword_encoded = test_data['keyword_lemma'].map(keyword_means).fillna(global_mean).values.reshape(-1, 1)\n",
    "\n",
    "# Construir matrices finales (DENSAS)\n",
    "X_train = np.hstack([\n",
    "    X_numeric[X_train_idx],\n",
    "    train_keyword_encoded,\n",
    "    X_text_dense[X_train_idx]\n",
    "])\n",
    "\n",
    "X_test = np.hstack([\n",
    "    X_numeric[X_test_idx],\n",
    "    test_keyword_encoded,\n",
    "    X_text_dense[X_test_idx]\n",
    "])\n",
    "\n",
    "y_train = y[X_train_idx]\n",
    "y_test = y[X_test_idx]\n",
    "\n",
    "print(f'Shape de X_train: {X_train.shape}')\n",
    "print(f'Shape de X_test: {X_test.shape}')\n",
    "print(f'Tipo de matriz: {type(X_train)} (densa)')\n",
    "\n",
    "# Modelo XGBoost con hiperparámetros ajustados para datos densos\n",
    "param_distributions = {\n",
    "    'max_depth': [6, 8, 10, 12],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [300, 500, 800],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.5, 0.7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "xgb = XGBClassifier(random_state=SEED, eval_metric='logloss')\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb, \n",
    "    param_distributions, \n",
    "    n_iter=50,\n",
    "    cv=skf, \n",
    "    scoring=make_scorer(f1_score), \n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    random_state=SEED\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Mejores parámetros: {random_search.best_params_}')\n",
    "print(f'Mejor F1-Score (CV): {random_search.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b5545",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c81072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score (Test, threshold=0.5): 0.7484\n",
      "F1-Score (Test, threshold=0.40): 0.7541\n",
      "Mejora: +0.0057\n"
     ]
    }
   ],
   "source": [
    "# Optimización del threshold para maximizar F1-Score\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predecir probabilidades\n",
    "y_probs_train = best_model.predict_proba(X_train)[:, 1]\n",
    "y_probs_test = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Buscar mejor threshold\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "\n",
    "for thresh in np.arange(0.3, 0.7, 0.01):\n",
    "    y_pred_thresh = (y_probs_test > thresh).astype(int)\n",
    "    score = f1_score(y_test, y_pred_thresh)\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_thresh = thresh\n",
    "\n",
    "# F1 con threshold por defecto (0.5)\n",
    "y_pred_default = best_model.predict(X_test)\n",
    "f1_default = f1_score(y_test, y_pred_default)\n",
    "\n",
    "# F1 con threshold optimizado\n",
    "y_pred_optimized = (y_probs_test > best_thresh).astype(int)\n",
    "\n",
    "print(f'F1-Score (Test, threshold=0.5): {f1_default:.4f}')\n",
    "print(f'F1-Score (Test, threshold={best_thresh:.2f}): {best_f1:.4f}')\n",
    "print(f'Mejora: +{(best_f1 - f1_default):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
