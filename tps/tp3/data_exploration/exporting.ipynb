{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626b4d48",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline\n",
    "\n",
    "Este notebook aplica todas las transformaciones de feature engineering desarrolladas durante la exploración y exporta los datasets procesados para su uso en modelos de machine learning.\n",
    "\n",
    "Postdata: Todo esto fue generado con inteligencia artificial a partir del notebook de exploración de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba8490",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07829eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import spacy\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2480d4b",
   "metadata": {},
   "source": [
    "## Cargar Modelo de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6506292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado: core_web_sm v3.8.0\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo de spacy para lematización\n",
    "model = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "print(f\"Modelo cargado: {model.meta['name']} v{model.meta['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823622e",
   "metadata": {},
   "source": [
    "## Funciones de Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70864520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    \"\"\"Limpia el texto del tweet removiendo menciones, URLs, emojis y caracteres especiales\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', '', text)                # menciones\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)     # URLs\n",
    "    text = emoji.replace_emoji(text, replace=' ')   # emojis\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)            # símbolos especiales\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # espacios extras\n",
    "    return text\n",
    "\n",
    "def lemma_filter(token):\n",
    "    \"\"\"Filtra stopwords y tokens de longitud 1\"\"\"\n",
    "    return token.lemma_ not in stop_words and len(token.lemma_) > 1\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lematiza el texto del tweet\"\"\"\n",
    "    cleaned_text = clean_tweet(text)\n",
    "    doc = model(cleaned_text)\n",
    "    return ' '.join([token.lemma_ for token in doc if lemma_filter(token)])\n",
    "\n",
    "def add_text_features(df):\n",
    "    \"\"\"Agrega features basados en el análisis del texto\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Features básicos de texto\n",
    "    df[\"text_length\"] = df[\"text\"].str.len()\n",
    "    df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
    "    df[\"hashtag_count\"] = df[\"text\"].str.count(\"#\")\n",
    "    df[\"mention_count\"] = df[\"text\"].str.count(\"@\")\n",
    "    df[\"url_count\"] = df[\"text\"].str.count(\"http\")\n",
    "    df[\"uppercase_percentage\"] = (\n",
    "        df[\"text\"].str.findall(r\"[A-Z]\").str.len() / df[\"text_length\"]\n",
    "    )\n",
    "    df[\"punctuation_percentage\"] = (\n",
    "        df[\"text\"].str.findall(r\"[.,!?\\\"\\'()]\").str.len() / df[\"text_length\"]\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_keyword_features(df):\n",
    "    \"\"\"Limpia y procesa las keywords\"\"\"\n",
    "    df = df.copy()\n",
    "    df['keyword_clean'] = df['keyword'].str.lower().str.replace('%20', ' ')\n",
    "    return df\n",
    "\n",
    "def add_lemmatization_features(df):\n",
    "    \"\"\"Agrega el texto lematizado\"\"\"\n",
    "    df = df.copy()\n",
    "    print(\"Lematizando textos...\")\n",
    "    df['text_lemmatized'] = df['text'].apply(lemmatize_text)\n",
    "    print(\"Lematización completada!\")\n",
    "    return df\n",
    "\n",
    "def apply_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Aplica todo el pipeline de feature engineering al dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset original con columnas: id, keyword, location, text (y target si es train)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataset con todas las features agregadas\n",
    "    \"\"\"\n",
    "    print(f\"Dataset original: {df.shape}\")\n",
    "    \n",
    "    # Aplicar transformaciones\n",
    "    df = add_text_features(df)\n",
    "    print(\"✓ Features de texto agregados\")\n",
    "    \n",
    "    df = add_keyword_features(df)\n",
    "    print(\"✓ Keywords procesados\")\n",
    "    \n",
    "    df = add_lemmatization_features(df)\n",
    "    print(\"✓ Lematización completada\")\n",
    "    \n",
    "    print(f\"\\nDataset final: {df.shape}\")\n",
    "    print(f\"Nuevas columnas agregadas: {df.shape[1] - 5}\")  # 5 columnas originales (id, keyword, location, text, target)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738ee46",
   "metadata": {},
   "source": [
    "## Cargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa3a2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (7613, 5)\n",
      "Test dataset: (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"../.data/raw/\"\n",
    "\n",
    "train_tweets = pd.read_csv(BASE_PATH + \"train.csv\")\n",
    "test_tweets = pd.read_csv(BASE_PATH + \"test.csv\")\n",
    "\n",
    "print(f\"Train dataset: {train_tweets.shape}\")\n",
    "print(f\"Test dataset: {test_tweets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7804c",
   "metadata": {},
   "source": [
    "## Aplicar Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3b313",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808cb4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PROCESANDO TRAIN DATASET\n",
      "==================================================\n",
      "Dataset original: (7613, 5)\n",
      "✓ Features de texto agregados\n",
      "✓ Keywords procesados\n",
      "Lematizando textos...\n",
      "Lematización completada!\n",
      "✓ Lematización completada\n",
      "\n",
      "Dataset final: (7613, 14)\n",
      "Nuevas columnas agregadas: 9\n",
      "Lematización completada!\n",
      "✓ Lematización completada\n",
      "\n",
      "Dataset final: (7613, 14)\n",
      "Nuevas columnas agregadas: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>uppercase_percentage</th>\n",
       "      <th>punctuation_percentage</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>4184</td>\n",
       "      <td>drown</td>\n",
       "      <td>Jonesboro, Arkansas USA</td>\n",
       "      <td>We are getting some reports of flooding near J...</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>drown</td>\n",
       "      <td>report flooding near jonesboro high school use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>728</td>\n",
       "      <td>attacked</td>\n",
       "      <td>#GDJB #ASOT</td>\n",
       "      <td>@eunice_njoki aiii she needs to chill and answ...</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>attacked</td>\n",
       "      <td>aiii need chill answer calmly like attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>4812</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FAAN orders evacuation of abandoned aircraft a...</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>faan order evacuation abandon aircraft mma faa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4410</th>\n",
       "      <td>6269</td>\n",
       "      <td>hijacking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vehicle Hijacking in  Vosloorus Gauteng on 201...</td>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>hijacking</td>\n",
       "      <td>vehicle hijacking vosloorus gauteng 2015 08 05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>6392</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Hurricane_Dolce happy birthday big Bruh</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>happy birthday big bruh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     keyword                 location  \\\n",
       "2915  4184       drown  Jonesboro, Arkansas USA   \n",
       "503    728    attacked              #GDJB #ASOT   \n",
       "3361  4812  evacuation                      NaN   \n",
       "4410  6269   hijacking                      NaN   \n",
       "4495  6392   hurricane                      NaN   \n",
       "\n",
       "                                                   text  target  text_length  \\\n",
       "2915  We are getting some reports of flooding near J...       1          137   \n",
       "503   @eunice_njoki aiii she needs to chill and answ...       0           89   \n",
       "3361  FAAN orders evacuation of abandoned aircraft a...       1          136   \n",
       "4410  Vehicle Hijacking in  Vosloorus Gauteng on 201...       1          117   \n",
       "4495           @Hurricane_Dolce happy birthday big Bruh       0           40   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  url_count  \\\n",
       "2915          23              0              0          0   \n",
       "503           15              0              1          0   \n",
       "3361          20              0              0          1   \n",
       "4410          14              0              0          1   \n",
       "4495           5              0              1          0   \n",
       "\n",
       "      uppercase_percentage  punctuation_percentage keyword_clean  \\\n",
       "2915              0.043796                0.029197         drown   \n",
       "503               0.000000                0.011236      attacked   \n",
       "3361              0.125000                0.029412    evacuation   \n",
       "4410              0.128205                0.008547     hijacking   \n",
       "4495              0.075000                0.000000     hurricane   \n",
       "\n",
       "                                        text_lemmatized  \n",
       "2915  report flooding near jonesboro high school use...  \n",
       "503           aiii need chill answer calmly like attack  \n",
       "3361  faan order evacuation abandon aircraft mma faa...  \n",
       "4410  vehicle hijacking vosloorus gauteng 2015 08 05...  \n",
       "4495                            happy birthday big bruh  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PROCESANDO TRAIN DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_processed = apply_feature_engineering(train_tweets)\n",
    "train_processed.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee1d7d",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "159f31c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PROCESANDO TEST DATASET\n",
      "==================================================\n",
      "Dataset original: (3263, 4)\n",
      "✓ Features de texto agregados\n",
      "✓ Keywords procesados\n",
      "Lematizando textos...\n",
      "Lematización completada!\n",
      "✓ Lematización completada\n",
      "\n",
      "Dataset final: (3263, 13)\n",
      "Nuevas columnas agregadas: 8\n",
      "Lematización completada!\n",
      "✓ Lematización completada\n",
      "\n",
      "Dataset final: (3263, 13)\n",
      "Nuevas columnas agregadas: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>uppercase_percentage</th>\n",
       "      <th>punctuation_percentage</th>\n",
       "      <th>keyword_clean</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>7422</td>\n",
       "      <td>obliterated</td>\n",
       "      <td>USA</td>\n",
       "      <td>the 301+ feature on YouTube has been obliterat...</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>obliterated</td>\n",
       "      <td>301 feature youtube obliterate love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>7100</td>\n",
       "      <td>military</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Boonie Hat USMC Airsoft Paintball Military...</td>\n",
       "      <td>116</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>military</td>\n",
       "      <td>new boonie hat usmc airsoft paintball military...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>975</td>\n",
       "      <td>blaze</td>\n",
       "      <td>Newburgh, NY</td>\n",
       "      <td>My big buzzy John BlaZe. Jus Kame home from a ...</td>\n",
       "      <td>97</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>blaze</td>\n",
       "      <td>big buzzy john blaze jus kame home 12 year bid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>4863</td>\n",
       "      <td>explode</td>\n",
       "      <td>sam</td>\n",
       "      <td>happy Justin makes my heart explode</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>explode</td>\n",
       "      <td>happy justin heart explode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>8915</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>@BigBang_CBS ...wow...ok...um...that was like ...</td>\n",
       "      <td>87</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>wow ok um like ice water blizzard snowstorm face</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword      location  \\\n",
       "2223  7422  obliterated          USA    \n",
       "2121  7100     military           NaN   \n",
       "300    975        blaze  Newburgh, NY   \n",
       "1466  4863      explode           sam   \n",
       "2672  8915    snowstorm   Los Angeles   \n",
       "\n",
       "                                                   text  text_length  \\\n",
       "2223  the 301+ feature on YouTube has been obliterat...           63   \n",
       "2121  New Boonie Hat USMC Airsoft Paintball Military...          116   \n",
       "300   My big buzzy John BlaZe. Jus Kame home from a ...           97   \n",
       "1466                happy Justin makes my heart explode           35   \n",
       "2672  @BigBang_CBS ...wow...ok...um...that was like ...           87   \n",
       "\n",
       "      word_count  hashtag_count  mention_count  url_count  \\\n",
       "2223          12              0              0          0   \n",
       "2121          13              0              0          2   \n",
       "300           15              1              0          1   \n",
       "1466           6              0              0          0   \n",
       "2672          11              0              1          0   \n",
       "\n",
       "      uppercase_percentage  punctuation_percentage keyword_clean  \\\n",
       "2223              0.047619                0.015873   obliterated   \n",
       "2121              0.206897                0.017241      military   \n",
       "300               0.113402                0.030928         blaze   \n",
       "1466              0.028571                0.000000       explode   \n",
       "2672              0.057471                0.149425     snowstorm   \n",
       "\n",
       "                                        text_lemmatized  \n",
       "2223                301 feature youtube obliterate love  \n",
       "2121  new boonie hat usmc airsoft paintball military...  \n",
       "300   big buzzy john blaze jus kame home 12 year bid...  \n",
       "1466                         happy justin heart explode  \n",
       "2672   wow ok um like ice water blizzard snowstorm face  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PROCESANDO TEST DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_processed = apply_feature_engineering(test_tweets)\n",
    "test_processed.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb8b24",
   "metadata": {},
   "source": [
    "## Exportar Datasets Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20d6be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guardando datasets procesados...\n",
      "✓ Train guardado en: ../.data/processed/train.pkl\n",
      "✓ Test guardado en: ../.data/processed/test.pkl\n",
      "==================================================\n",
      "EXPORTACIÓN COMPLETADA\n",
      "==================================================\n",
      "\n",
      "Archivos generados:\n",
      "  - ../.data/processed/train.pkl (1.87 MB)\n",
      "  - ../.data/processed/test.pkl (0.79 MB)\n"
     ]
    }
   ],
   "source": [
    "# Crear directorio de salida si no existe\n",
    "OUTPUT_PATH = Path(\"../.data/processed/\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Exportar a pickle\n",
    "train_output = OUTPUT_PATH / \"train.pkl\"\n",
    "test_output = OUTPUT_PATH / \"test.pkl\"\n",
    "\n",
    "print(\"\\nGuardando datasets procesados...\")\n",
    "train_processed.to_pickle(train_output)\n",
    "print(f\"✓ Train guardado en: {train_output}\")\n",
    "\n",
    "test_processed.to_pickle(test_output)\n",
    "print(f\"✓ Test guardado en: {test_output}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"EXPORTACIÓN COMPLETADA\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nArchivos generados:\")\n",
    "print(f\"  - {train_output} ({train_output.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"  - {test_output} ({test_output.stat().st_size / 1024 / 1024:.2f} MB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
